{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51eb4028",
   "metadata": {},
   "source": [
    "## 0. Import Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8bd1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests as r\n",
    "import time\n",
    "import random\n",
    "import copy\n",
    "import pgeocode\n",
    "import pickle\n",
    "import spacy\n",
    "import csv\n",
    "import json\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup as soup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452ca226",
   "metadata": {},
   "source": [
    "## 1. Set the Landing Page for Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdd2a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Landing page (p1) - Restaurants with \"Establishment Type\" = Coffee & Tea in London\n",
    "url_land = 'https://www.tripadvisor.co.uk/Restaurants-g186338-zfg9900-London_England.html'\n",
    "url_root = 'https://www.tripadvisor.co.uk'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a959c5f",
   "metadata": {},
   "source": [
    "## 2. Create a dictionary of London Cafes\n",
    "### 2. 1 Gather information from the search results for London Cafes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ca7043",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create empty dict for cafe info\n",
    "cafedic = {}\n",
    "\n",
    "# Set starting page number\n",
    "i = 1\n",
    "\n",
    "# Limit of 100 pages of cafes\n",
    "while i <= 100:\n",
    "\n",
    "    # For the landing page\n",
    "    if i == 1:\n",
    "        page = url_land\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    # Collect page content\n",
    "    response = r.get(page, headers={'User-Agent': \"Mozilla/5.0\"})\n",
    "    content = soup(response.content, 'lxml')\n",
    "\n",
    "    # Get cafe list\n",
    "    content_cafe = content.find('div', {'class': 'YtrWs'})\n",
    "\n",
    "    # Create restaurant dict\n",
    "    temp_i = 0\n",
    "    for t in content_cafe.find_all('a'):\n",
    "        # Extract the text\n",
    "        txt = t.text\n",
    "        # Get info on names and # of reviews\n",
    "        excl = ['“', \"Reserve\", \"Order\"]\n",
    "        if len(txt) > 0 and not any(e in txt for e in excl):\n",
    "            txt = txt.split(\" \",1)\n",
    "            txt[0] = txt[0].replace(\".\",\"\").replace(\",\",\"\")\n",
    "            if 'review' not in txt[1]:\n",
    "                txt[0] = int(txt[0])\n",
    "                # Create dict with rank as id\n",
    "                cafedic.setdefault(txt[0], {})\n",
    "                # Create nested dict and add name and cafe link\n",
    "                cafedic[txt[0]]['name'] = txt[1]\n",
    "                cafedic[txt[0]]['cafeurl'] = url_root + t['href']\n",
    "                temp_i = txt[0]\n",
    "            else:\n",
    "                # Add # of reviews to nested dict (0 if no reviews)\n",
    "                try:\n",
    "                    cafedic[temp_i]['reviewcnt'] = int(txt[0])\n",
    "                except:\n",
    "                    cafedic[temp_i]['reviewcnt'] = 0\n",
    "\n",
    "    # Update page link to next page\n",
    "    try:\n",
    "        i += 1\n",
    "        page = url_root + content.find('a', {'data-page-number' : i})['href']\n",
    "        # To avoid being blocked for scraping\n",
    "        time.sleep(random.randint(1, 3))\n",
    "\n",
    "    # Until we have gone through every page\n",
    "    except:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdce68f",
   "metadata": {},
   "source": [
    "### 2.2 Gather information from each of the Cafes' Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59b248e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cafeids = copy.deepcopy(list(cafedic.keys()))\n",
    "reviewcats = ['Excellent', 'Very good', 'Average', 'Poor', 'Terrible']\n",
    "\n",
    "for i in cafeids:\n",
    "    print(i)\n",
    "    url =  cafedic[i]['cafeurl']\n",
    "    \n",
    "    # Collect review page content\n",
    "    response = r.get(url, headers={'User-Agent': \"Mozilla/5.0\"})\n",
    "    content = soup(response.content,'lxml')\n",
    "    \n",
    "    # Add cafe rating\n",
    "    try:\n",
    "        cafedic[i]['rating'] = float(content.find('h2').find_next('span').text)\n",
    "    except:\n",
    "        cafedic[i]['rating'] = np.nan\n",
    "        \n",
    "    # Add review counts per category\n",
    "    try:\n",
    "        catcnts = [int(i.text.replace(',','')) for i in content.find_all('span', {'class': 'row_num is-shown-at-tablet'})]\n",
    "        for cat in range(len(reviewcats)):\n",
    "            attrname = 'rev_'+reviewcats[cat]\n",
    "            cafedic[i][attrname] = catcnts[cat] \n",
    "\n",
    "        # Add the category with the most reviews\n",
    "        topcat = reviewcats[catcnts.index(max(catcnts))]\n",
    "        cafedic[i]['topcat'] = topcat\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    # Add price range\n",
    "    try:\n",
    "        if '£' not in content.find('a', {'class': 'dlMOJ'}).text:\n",
    "            raise AttributeError\n",
    "        else:\n",
    "            cafedic[i]['pricerng'] = content.find('a', {'class': 'dlMOJ'}).text\n",
    "            \n",
    "    except AttributeError:\n",
    "        cafedic[i]['pricerng'] = np.nan\n",
    "    \n",
    "    # Add cafe address\n",
    "    try:\n",
    "        fullad = content.find(text = 'Location and contact').find_next('span', {'class' : 'yEWoV'}).get_text().split()\n",
    "        postcode = ' '.join(fullad[-3:-1])\n",
    "        if len(postcode) > 8:\n",
    "            raise AttributeError\n",
    "        else:\n",
    "            cafedic[i]['postcode'] = postcode\n",
    "    except AttributeError:\n",
    "        cafedic[i]['postcode'] = np.nan\n",
    "        \n",
    "    # Add borough\n",
    "    try:\n",
    "        pc = postcode.replace(' ','')\n",
    "        url = f'http://api.postcodes.io/postcodes/{pc}'\n",
    "        borough_content = r.get(url)\n",
    "        cafedic[i]['borough'] = json.loads(borough_content.text)['result']['primary_care_trust']\n",
    "    except:\n",
    "            cafedic[i]['borough'] = np.nan\n",
    "    \n",
    "    # Add latitude\n",
    "    nomi = pgeocode.Nominatim('GB', 'fr')\n",
    "    try:\n",
    "        cafedic[i]['lat'] = nomi.query_postal_code(f'{postcode}')[9]\n",
    "    except ValueError:\n",
    "        cafedic[i]['lat'] = np.nan\n",
    "\n",
    "    # Add longitude\n",
    "    try:\n",
    "        cafedic[i]['long'] = nomi.query_postal_code(f'{postcode}')[10]\n",
    "    except ValueError:\n",
    "        cafedic[i]['long'] = np.nan       \n",
    "    \n",
    "    # To avoid being blocked for scraping\n",
    "    time.sleep(random.randint(1, 3))\n",
    "\n",
    "# Write cafe dictionary to binary file\n",
    "file = open(\"cafedict\", \"wb\")\n",
    "pickle.dump(cafedic, file)\n",
    "file.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229a1b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) For reading pre-formed cafe data\n",
    "\n",
    "cafedic = pd.read_pickle(r'cafedict')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45a1e71",
   "metadata": {},
   "source": [
    "## 3. Create a table with review data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3214723c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Reset review list\n",
    "revlst = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bdf48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make review list\n",
    "\n",
    "cafeids = copy.deepcopy(list(cafedic.keys()))\n",
    "\n",
    "with open(\"reviews.csv\", \"w\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "\n",
    "    for c in cafeids:\n",
    "\n",
    "        url =  cafedic[c]['cafeurl']\n",
    "\n",
    "        # Collect review page content\n",
    "        response = r.get(url, headers={'User-Agent': \"Mozilla/5.0\"})\n",
    "        content = soup(response.content,'lxml') \n",
    "\n",
    "\n",
    "        # Collect details for 15 most recent reviews\n",
    "    \n",
    "        # Review\n",
    "        try:\n",
    "            revinfo = content.find_all('div', {'class' : 'ui_column is-9'})\n",
    "            # Date\n",
    "            date = content.find_all('span', {'class' : 'ratingDate'})\n",
    "            \n",
    "            try:\n",
    "                revlst = []\n",
    "                for rev in revinfo:\n",
    "                    revtxt = rev.find('p', {'class' : 'partial_entry'})\n",
    "                    if revtxt:\n",
    "                        revlst.append(revtxt.text)\n",
    "            except:\n",
    "                print(\"no revs\")\n",
    "                pass\n",
    "            \n",
    "            # Rating\n",
    "            try:\n",
    "                ratlst = []\n",
    "                for rat in revinfo:\n",
    "                    ratscore = rat.find('span', {'class': 'ui_bubble_rating'})\n",
    "                    if ratscore:\n",
    "                        ratlst.append(int(ratscore['class'][1][-2:])/10)\n",
    "            except:\n",
    "                print(\"no ratings\")\n",
    "                pass\n",
    "\n",
    "            # Add details to list / csv\n",
    "            for rev in range(len(date)):\n",
    "                rev_details = []\n",
    "                \n",
    "                # Add the cafe id\n",
    "                rev_details.append(c)\n",
    "                \n",
    "                # Add date\n",
    "                rev_details.append(pd.to_datetime(date[rev]['title']).date())\n",
    "\n",
    "                # Review rating\n",
    "                try:\n",
    "                    rev_details.append(ratlst[rev])\n",
    "                except:\n",
    "                    print(\"no rating\")\n",
    "                    rev_details.append(np.nan)\n",
    "                    pass\n",
    "                \n",
    "                # Review content\n",
    "                try:\n",
    "                    rev_details.append(revlst[rev])\n",
    "                except:\n",
    "                    print(\"no text\")\n",
    "                    rev_details.append(np.nan)\n",
    "                    pass\n",
    "                \n",
    "                # Write to csv / add to list\n",
    "                writer.writerow(rev_details)\n",
    "                revlst.append(rev_details)\n",
    "                \n",
    "        except:\n",
    "            print(\"No reviews found\", c)\n",
    "            pass\n",
    "        \n",
    "        print(\"done:\",c)\n",
    "        # To avoid being blocked for scraping\n",
    "        time.sleep(random.randint(1, 3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
